{"cells":[{"cell_type":"markdown","metadata":{"id":"0qeVgYvCCViV"},"source":["# Procesamiento del Lenguaje Natural (NLP) usando RNNs"]},{"cell_type":"markdown","metadata":{"id":"VQOAa_91CViW"},"source":["Gran parte del código que aparece a continuación procede del [repositorio de A. Géron](http://github/ageron/handson-ml2/). Muchas gracias a él."]},{"cell_type":"markdown","metadata":{"id":"6VoctkTrCViX"},"source":["# Celdas preparatorias"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"mkzHnrz2CViX","executionInfo":{"status":"ok","timestamp":1696448212010,"user_tz":180,"elapsed":4332,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["# Se requiere Python ≥3.5\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","# El notebook se está ejecutando en Colab o en Kaggle?\n","IS_COLAB = \"google.colab\" in sys.modules\n","IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n","\n","# if IS_COLAB:\n","#     %pip install -q -U tensorflow-addons\n","#     %pip install -q -U transformers\n","\n","# Se requiere Scikit-Learn ≥0.20\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# Se requiere TensorFlow ≥2.0\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","tf.keras.backend.clear_session()\n","\n","if not tf.config.list_physical_devices('GPU'):\n","    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n","    if IS_COLAB:\n","        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n","    if IS_KAGGLE:\n","        print(\"Go to Settings > Accelerator and select GPU.\")\n","\n","# Importaciones comunes\n","import numpy as np\n","import os\n","\n","# Para hacer que las salidas sean estable en distintas ejecuciones\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# Plotear imágenes lindas\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","# Donde salvar las figuras\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"nlp\"\n","IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n","os.makedirs(IMAGES_PATH, exist_ok=True)\n","\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format=fig_extension, dpi=resolution)"]},{"cell_type":"markdown","metadata":{"id":"WZA0KCe1CViY"},"source":["# De a un caracter"]},{"cell_type":"markdown","metadata":{"id":"B9DjGeosCViY"},"source":["La  tarea de PLN que emprenderemos es la predicción/producción de texto de un carácter a la vez. En otras palabras, proporcionaremos al modelo un texto como \"Hola, mund\" y esperaremos que la predicción sea un solo carácter, \"o\".\n","\n","Como veremos, la principal dificultad es la preparación de los datos; el resto son RNN a la antigua usanza. Vamos a sumergirnos en la clase `Dataset` de `Tensorflow`."]},{"cell_type":"markdown","metadata":{"id":"Ltiyw2P8CViY"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"hKZEzKUeCViY"},"source":["Hay varias maneras de crear un objeto `Dataset`, pero quizás la más fácil es usar `from_tensor_slices`."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lt9cHhCyCViY","executionInfo":{"status":"ok","timestamp":1696448250214,"user_tz":180,"elapsed":276,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"6ddc1490-d917-4adf-d5cc-23dbb06bb4d7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"]},"metadata":{},"execution_count":2}],"source":["X = tf.range(10)\n","dataset = tf.data.Dataset.from_tensor_slices(X)\n","dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"layukaIACViY","executionInfo":{"status":"ok","timestamp":1696448253575,"user_tz":180,"elapsed":260,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"240f3584-96c9-4ad7-f625-91324b27454c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"]},"metadata":{},"execution_count":3}],"source":["X"]},{"cell_type":"markdown","metadata":{"id":"tYDRfgtmCViY"},"source":["De forma equivalente:"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zKAqMhjUCViY","executionInfo":{"status":"ok","timestamp":1696448256961,"user_tz":180,"elapsed":276,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"03cc0418-21c9-4805-ec1a-553274291949"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_RangeDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"]},"metadata":{},"execution_count":4}],"source":["dataset = tf.data.Dataset.range(10)\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"rBVFuqWECViY"},"source":["En cualquier caso, la instancia resultante puede utilizarse como un iterador"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Spm2VLx5CViY","executionInfo":{"status":"ok","timestamp":1696448260199,"user_tz":180,"elapsed":267,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"bd919837-12ff-4da5-f5f0-be967c6093f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(0, shape=(), dtype=int64)\n","tf.Tensor(1, shape=(), dtype=int64)\n","tf.Tensor(2, shape=(), dtype=int64)\n","tf.Tensor(3, shape=(), dtype=int64)\n","tf.Tensor(4, shape=(), dtype=int64)\n","tf.Tensor(5, shape=(), dtype=int64)\n","tf.Tensor(6, shape=(), dtype=int64)\n","tf.Tensor(7, shape=(), dtype=int64)\n","tf.Tensor(8, shape=(), dtype=int64)\n","tf.Tensor(9, shape=(), dtype=int64)\n"]}],"source":["for item in dataset:\n","    print(item)"]},{"cell_type":"markdown","metadata":{"id":"ZZOeyFQDCViY"},"source":["#### Repeat y Batch"]},{"cell_type":"markdown","metadata":{"id":"5mGdaFcDCViY"},"source":["Esta clase tiene muchos métodos interesantes, como `repeat` o `batch`."]},{"cell_type":"code","execution_count":6,"metadata":{"tags":["raises-exception"],"colab":{"base_uri":"https://localhost:8080/"},"id":"LPNBXQs6CViY","executionInfo":{"status":"ok","timestamp":1696448264153,"user_tz":180,"elapsed":273,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"5cf39d85-58f2-4f34-ea51-4158f86b38e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n","tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n","tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n","tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n"]}],"source":["dataset = dataset.repeat(3).batch(7, drop_remainder=True)\n","for item in dataset:\n","    print(item)"]},{"cell_type":"markdown","metadata":{"id":"a61pTq8XCViY"},"source":["### Map"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"MyVMaYmrCViY","executionInfo":{"status":"ok","timestamp":1696448267382,"user_tz":180,"elapsed":265,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["dataset = dataset.map(lambda x: x * 2)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ye2vaMkXCViY","executionInfo":{"status":"ok","timestamp":1696448269790,"user_tz":180,"elapsed":271,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"96967733-7267-46da-a2da-78e3d92375eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n","tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n","tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n","tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n"]}],"source":["for item in dataset:\n","    print(item)"]},{"cell_type":"markdown","metadata":{"id":"F6OGx44JCViY"},"source":["### Unbatch"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"SOUiEi4hCViZ","executionInfo":{"status":"ok","timestamp":1696448272062,"user_tz":180,"elapsed":271,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["#dataset = dataset.apply(tf.data.experimental.unbatch()) # Now deprecated\n","dataset = dataset.unbatch()"]},{"cell_type":"markdown","metadata":{"id":"9qgPLhgnCViZ"},"source":["**Pregunta**. ¿Qué va a devolver el siguiente código?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05KTU8_tCViZ"},"outputs":[],"source":["for item in dataset:\n","    print(item)"]},{"cell_type":"markdown","metadata":{"id":"-3rrYT5VCViZ"},"source":["### Filter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOFLmD9vCViZ"},"outputs":[],"source":["dataset = dataset.filter(lambda x: x < 10)  # keep only items < 10"]},{"cell_type":"markdown","metadata":{"id":"mwS9ZpuyCViZ"},"source":["**Pregunta**. ¿Y ahora?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gj9x8J1_CViZ"},"outputs":[],"source":["for item in dataset:\n","    print(item)"]},{"cell_type":"markdown","metadata":{"id":"pqWbXDGQCViZ"},"source":["### Take"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z58RJ-NaCViZ"},"outputs":[],"source":["for item in dataset.take(3):\n","    print(item)"]},{"cell_type":"markdown","metadata":{"id":"LOzxvIUCCViZ"},"source":["### Shuffle"]},{"cell_type":"markdown","metadata":{"id":"zXqQAygyCViZ"},"source":["Miren cómo funciona `buffer_size`, que es bastante sutil; se toma un conjunto de buffer_size elementos, y se elije uno al azar; luego se tomar un conjunto de los siguientes buffer_size elementos, incluyendo los que no quedaron seleccionados en el primer caso, y se vuelve a elegir.\n","\n","Prueben distintos valores de este parámetro para entenderlo."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3WYrgUHCViZ","executionInfo":{"status":"ok","timestamp":1696448285183,"user_tz":180,"elapsed":272,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"b8031b38-5181-48d8-f96c-6f9015db33e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([5 4 7 3 6 5 5 2 2 7], shape=(10,), dtype=int64)\n","tf.Tensor([6 8 8 1 1 4 9 9 0 9], shape=(10,), dtype=int64)\n","tf.Tensor([3 0 2 8 0 1 3 7 4 6], shape=(10,), dtype=int64)\n"]}],"source":["tf.random.set_seed(42)\n","\n","dataset = tf.data.Dataset.range(10).repeat(3)\n","dataset = dataset.shuffle(buffer_size=3000, seed=42).batch(10)\n","for item in dataset:\n","    print(item)"]},{"cell_type":"markdown","metadata":{"id":"SdBI1sTNCViZ"},"source":["### Window"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlfgaOVZCViZ"},"outputs":[],"source":["dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n","\n","window_length = 3\n","dataset =  dataset.window(window_length, shift=4, drop_remainder=False)\n","for item in dataset:\n","    print(item)\n","    for ii in item:\n","        print(ii)"]},{"cell_type":"markdown","metadata":{"id":"7ZnlxI8ECViZ"},"source":["Fíjense la estructura *anidada* (*nested*) de este dataset."]},{"cell_type":"markdown","metadata":{"id":"2xKO-S0DCViZ"},"source":["### Flat map"]},{"cell_type":"markdown","metadata":{"id":"_cziD0MBCVia"},"source":["Este método es especialmente útil para conjuntos de datos anidados, como los anteriores. Transforma un conjunto de datos anidado en uno plano, pero aplicando antes una función a cada conjunto de datos anidado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VOle0-c0CVia"},"outputs":[],"source":["# Pasar la función identidad\n","for item in dataset.flat_map(lambda x: x):\n","    print(item)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fy9OTEbeCVih"},"outputs":[],"source":["# Pasar la función de batch con la longitud de la ventana nos da algo que usaremos más tarde.\n","for item in dataset.flat_map(lambda x: x.batch(window_length)):\n","    print(item)"]},{"cell_type":"markdown","metadata":{"id":"u01Z5pEKCVih"},"source":["***\n","\n","Listos para avanzar."]},{"cell_type":"markdown","metadata":{"id":"eouqGP2cCVih"},"source":["## Dividir una secuencia en lotes de ventanas ordenadas al azar"]},{"cell_type":"markdown","metadata":{"id":"p2rWd-1bCVih"},"source":["Por ejemplo, dividamos la secuencia 0 a 14 en ventanas de longitud 5, cada una desplazada por 2 (por ejemplo, `[0, 1, 2, 3, 4]`, `[2, 3, 4, 5, 6]`, etc.), luego barajémoslas, y dividámoslas en entradas (los primeros 4 pasos) y objetivos (los últimos 4 pasos) (por ejemplo `[2, 3, 4, 5, 6]` se dividiría en `[[2, 3, 4, 5], [3, 4, 5, 6]]`), y luego se crearían lotes de 3 de estos pares de entrada/objetivo:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"veooumTvCVih","executionInfo":{"status":"ok","timestamp":1696448313945,"user_tz":180,"elapsed":263,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","n_steps = 5\n","dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n","# dataset = tf.data.Dataset.range(15)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"IofmST-ACVih","executionInfo":{"status":"ok","timestamp":1696448315488,"user_tz":180,"elapsed":257,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["dataset = dataset.window(n_steps, shift=2, drop_remainder=False)\n","\n","# for item in dataset:\n","#     print(item)\n","#     for ii in item:\n","#         print(ii)\n",""]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6g5btzndCVih","executionInfo":{"status":"ok","timestamp":1696448318613,"user_tz":180,"elapsed":267,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"5c8796cc-cecb-4bef-c13d-1a3459306a11"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n","tf.Tensor([2 3 4 5 6], shape=(5,), dtype=int32)\n","tf.Tensor([4 5 6 7 8], shape=(5,), dtype=int32)\n","tf.Tensor([ 6  7  8  9 10], shape=(5,), dtype=int32)\n","tf.Tensor([ 8  9 10 11 12], shape=(5,), dtype=int32)\n","tf.Tensor([10 11 12 13 14], shape=(5,), dtype=int32)\n"]}],"source":["dataset = dataset.flat_map(lambda window: window.batch(n_steps, drop_remainder=True))\n","\n","for item in dataset:\n","    print(item)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FX25edwRCVih","executionInfo":{"status":"ok","timestamp":1696448321804,"user_tz":180,"elapsed":442,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"35161c61-6c74-4548-de0c-6e62553e154b"},"outputs":[{"output_type":"stream","name":"stdout","text":["(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([6, 7, 8, 9], dtype=int32)>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 7,  8,  9, 10], dtype=int32)>)\n","(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([2, 3, 4, 5], dtype=int32)>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 4, 5, 6], dtype=int32)>)\n","(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 5, 6, 7], dtype=int32)>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([5, 6, 7, 8], dtype=int32)>)\n","(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>)\n","(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 8,  9, 10, 11], dtype=int32)>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 9, 10, 11, 12], dtype=int32)>)\n","(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([10, 11, 12, 13], dtype=int32)>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([11, 12, 13, 14], dtype=int32)>)\n"]}],"source":["dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n","\n","for item in dataset:\n","    print(item)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhF2orwICVih","executionInfo":{"status":"ok","timestamp":1696448324897,"user_tz":180,"elapsed":265,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"f19a36b9-5bca-44f5-96d6-98360ddbde08"},"outputs":[{"output_type":"stream","name":"stdout","text":["_____ Batch 0 \n","X_batch\n","[[ 4  5  6  7]\n"," [ 0  1  2  3]\n"," [ 8  9 10 11]]\n","===== \n","Y_batch\n","[[ 5  6  7  8]\n"," [ 1  2  3  4]\n"," [ 9 10 11 12]]\n","_____ Batch 1 \n","X_batch\n","[[10 11 12 13]\n"," [ 2  3  4  5]\n"," [ 6  7  8  9]]\n","===== \n","Y_batch\n","[[11 12 13 14]\n"," [ 3  4  5  6]\n"," [ 7  8  9 10]]\n"]}],"source":["dataset = dataset.batch(3).prefetch(1)\n","for index, (X_batch, Y_batch) in enumerate(dataset):\n","    print(\"_\" * 5, \"Batch\", index, \"\\nX_batch\")\n","    print(X_batch.numpy())\n","    print(\"=\" * 5, \"\\nY_batch\")\n","    print(Y_batch.numpy())"]},{"cell_type":"code","execution_count":16,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"AVzz0qygCVih","executionInfo":{"status":"ok","timestamp":1696448327674,"user_tz":180,"elapsed":259,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"e7687b90-95ae-4ea5-de6d-3531b33d39af"},"outputs":[{"output_type":"stream","name":"stdout","text":["_____ Batch 0 \n","X_batch\n","[[6 7 8 9]\n"," [2 3 4 5]\n"," [4 5 6 7]]\n","===== \n","Y_batch\n","[[ 7  8  9 10]\n"," [ 3  4  5  6]\n"," [ 5  6  7  8]]\n","_____ Batch 1 \n","X_batch\n","[[ 0  1  2  3]\n"," [ 8  9 10 11]\n"," [10 11 12 13]]\n","===== \n","Y_batch\n","[[ 1  2  3  4]\n"," [ 9 10 11 12]\n"," [11 12 13 14]]\n"]}],"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","n_steps = 5\n","dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n","dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n","dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n","dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n","dataset = dataset.batch(3).prefetch(1)\n","for index, (X_batch, Y_batch) in enumerate(dataset):\n","    print(\"_\" * 5, \"Batch\", index, \"\\nX_batch\")\n","    print(X_batch.numpy())\n","    print(\"=\" * 5, \"\\nY_batch\")\n","    print(Y_batch.numpy())"]},{"cell_type":"markdown","metadata":{"id":"YEVzKa_pCVih"},"source":["## Cargamos los datos y preparamos el Dataset"]},{"cell_type":"markdown","metadata":{"id":"IoMsrq0QCVih"},"source":["Trabajaremos con las obras de Shakespeare del famoso [blog de Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtneTy6FCVih","executionInfo":{"status":"ok","timestamp":1696448331723,"user_tz":180,"elapsed":386,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"6882945c-0d13-4286-bc1c-8ed09f668115"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","1115394/1115394 [==============================] - 0s 0us/step\n"]}],"source":["shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n","filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n","with open(filepath) as f:\n","    shakespeare_text = f.read()"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3oXLki9VCVih","executionInfo":{"status":"ok","timestamp":1696448334245,"user_tz":180,"elapsed":3,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"3dcc8c87-9290-40a1-d353-6eca851d2d78"},"outputs":[{"output_type":"stream","name":"stdout","text":["ords.\n","\n","All The Lords:\n","You are most welcome home.\n","\n","AUFIDIUS:\n","I have not deserved it.\n","But, worthy lords, have you with heed perused\n","What I have writte\n"]}],"source":["print(shakespeare_text[150000:150000+148])"]},{"cell_type":"markdown","metadata":{"id":"KPSyGUsICVih"},"source":["Veamos qué caracteres aparecen en el texto."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"31DMW6LFCVih","executionInfo":{"status":"ok","timestamp":1696448337494,"user_tz":180,"elapsed":250,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"9b4030ec-d852-4f76-e43e-fdccb2d0f686"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1115394"]},"metadata":{},"execution_count":19}],"source":["len(shakespeare_text)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"OIL1iU52CVih","executionInfo":{"status":"ok","timestamp":1696448339395,"user_tz":180,"elapsed":254,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"74dc7ea7-9084-4941-bcee-7964c1bc4204"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n   ! $ & ' , - . 3 : ; ? a b c d e f g h i j k l m n o p q r s t u v w x y z\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}],"source":["\" \".join(sorted(set(shakespeare_text.lower())))"]},{"cell_type":"markdown","metadata":{"id":"7Xo6Q_OsCVii"},"source":["Primero debemos convertir estos caracteres en números. Esto es muy fácil de hacer con la clase `Tokenizer` que también es útil para muchas otras tareas de PNL.\n","\n","Aquí lo utilizamos con el argumento `char_level` establecido en `True`, de modo que cada carácter del texto recibe un token, y lo ajustamos utilizando todo el conjunto de datos."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"AXXLVlksCVii","executionInfo":{"status":"ok","timestamp":1696448344369,"user_tz":180,"elapsed":1000,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["tokenizer = keras.preprocessing.text.Tokenizer(char_level=True, lower=False)\n","tokenizer.fit_on_texts(shakespeare_text)"]},{"cell_type":"markdown","metadata":{"id":"wK-J5AnUCVii"},"source":["Observe los dos métodos para ir y venir entre los tokens y los caracteres reales. Observen también que hemos descuidado completamente la diferencia entre las letras minúsculas y las mayúsculas. El uso de `lower=False` en el tokenizador revierte esto."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkGNu10nCVii","executionInfo":{"status":"ok","timestamp":1696448346018,"user_tz":180,"elapsed":5,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"aefba1f9-2750-45c4-de1d-ad8e4d8e7d5b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[50, 10, 8, 7, 3]]"]},"metadata":{},"execution_count":22}],"source":["tokenizer.texts_to_sequences([\"First\"])"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7qaNKEJCVii","executionInfo":{"status":"ok","timestamp":1696448349119,"user_tz":180,"elapsed":3,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"fbeba64c-c0bf-42b7-f19e-8fc8b0310427"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['F i r s t']"]},"metadata":{},"execution_count":23}],"source":["# tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])\n","tokenizer.sequences_to_texts([[50, 10, 8, 7, 3]])"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ihEk3dgnCVii","executionInfo":{"status":"ok","timestamp":1696448352033,"user_tz":180,"elapsed":7,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"1a864b70-cd6c-45c8-90f4-3d100fcb4d09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hay 65 caracteres diferentes en el texto\n","Hay un total de 1115394  caracteres en el texto\n"]}],"source":["max_id = len(tokenizer.word_index) # number of distinct characters\n","dataset_size = tokenizer.document_count # total number of characters\n","\n","print('Hay {} caracteres diferentes en el texto'.format(max_id))\n","print('Hay un total de {}  caracteres en el texto'.format(dataset_size))"]},{"cell_type":"markdown","metadata":{"id":"pOdv2k7PCVii"},"source":["Ahora vamos a tokenizar el arte shakesperiano y convertirlo en un elemento para *nuestro* arte (ahre)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"RvOzBI4UCVii","executionInfo":{"status":"ok","timestamp":1696448355980,"user_tz":180,"elapsed":691,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n","train_size = dataset_size * 90 // 100\n","dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"]},{"cell_type":"markdown","metadata":{"id":"7YZ7NPSDCVii"},"source":["Ahora viene la parte complicada. Es imposible entrenar una RNN con una sola instancia de más de 1 millón de caracteres. En su lugar, dividiremos el texto en pequeñas frases de unos 100 caracteres y las utilizaremos para entrenar la RNN.\n","\n","Esta es la idea:\n","\n","![Imagen](../images/Figure16-1_geron.png)\n","\n","N.B.: ya puedes adivinar cómo se utilizará el método `window`.\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"fCGkg6t3CVii","executionInfo":{"status":"ok","timestamp":1696448365691,"user_tz":180,"elapsed":259,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["n_steps = 100\n","window_length = n_steps + 1 # target = input shifted 1 character ahead\n","dataset = dataset.window(window_length, shift=1, drop_remainder=True)"]},{"cell_type":"markdown","metadata":{"id":"Mhx9mrWqCVii"},"source":["Ok, eso fue fácil... pero la RNN no podrá entrenar con un *conjunto de datos anidados*, así que necesitamos usar `flat_map` como antes."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"SioxHpuGCVii","executionInfo":{"status":"ok","timestamp":1696448374336,"user_tz":180,"elapsed":262,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["dataset = dataset.flat_map(lambda window: window.batch(window_length))"]},{"cell_type":"markdown","metadata":{"id":"YmBxTsHECVii"},"source":["Ahora shuffle, batchear y construir las \"características\" y la etiqueta (es decir, los 100 primeros caracteres y el carácter siguiente correspondiente)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"1eDoY8FaCVii","executionInfo":{"status":"ok","timestamp":1696448375346,"user_tz":180,"elapsed":2,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["# np.random.seed(42)\n","# tf.random.set_seed(42)\n","batch_size = 32\n","dataset = dataset.shuffle(10000).batch(batch_size)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"ZxA_-KqkCVii","executionInfo":{"status":"ok","timestamp":1696448377771,"user_tz":180,"elapsed":255,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"]},{"cell_type":"markdown","metadata":{"id":"P4yiegiYCVii"},"source":["Ponemos todo esto en una práctica función."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"fgGSqqX0CVii","executionInfo":{"status":"ok","timestamp":1696448378976,"user_tz":180,"elapsed":5,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n","    ds = tf.data.Dataset.from_tensor_slices(sequence)\n","    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n","    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n","    if shuffle:\n","        ds = ds.shuffle(100_000, seed=seed)\n","    ds = ds.batch(batch_size)\n","    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"]},{"cell_type":"markdown","metadata":{"id":"87_5Aa91CVii"},"source":["Además, seguimos tratando con *tokens* (números), que son una variable categórica. Normalmente es necesario codificarlos. Aquí utilizamos la codificación de un solo punto (no hay tantos caracteres distintos). Introduzca el método `map`."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"0YVdBFiqCVii","executionInfo":{"status":"ok","timestamp":1696448381646,"user_tz":180,"elapsed":385,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"]},{"cell_type":"markdown","metadata":{"id":"jmW3QJDyCVij"},"source":["Veamos lo que tenemos. Agarramos el primer lote y miramos las formas."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g49b4m7FCVij","executionInfo":{"status":"ok","timestamp":1696448384673,"user_tz":180,"elapsed":1874,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"31d4beb1-45e0-47e1-feb7-8173d0638ce7"},"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 100, 65) (32, 100)\n"]}],"source":["dataset = dataset.prefetch(1)\n","\n","for X_batch, Y_batch in dataset.take(1):\n","    print(X_batch.shape, Y_batch.shape)"]},{"cell_type":"markdown","metadata":{"id":"Kl18ESR6CVij"},"source":["Así pues, tenemos 32 instancias, de cien caracteres, cada una de las cuales es un vector de 32 elementos (sólo uno de los cuales no es cero).\n","\n","Los *targets* no están codificados."]},{"cell_type":"markdown","metadata":{"id":"8a3WQs6uCVij"},"source":["## Creación y entrenamiento del modelo"]},{"cell_type":"markdown","metadata":{"id":"MV-pKlJdCVij"},"source":["**Atención**: el siguiente código puede tardar hasta 24 horas en ejecutarse, dependiendo de tu hardware. Si usas una GPU, puede tardar solo 1 o 2 horas, o menos."]},{"cell_type":"markdown","metadata":{"id":"YW5gAvoXCVij"},"source":["**Nota de Géron**: la clase `GRU` sólo utilizará la GPU (si tiene una) cuando utilice los valores por defecto para los siguientes argumentos: `activation`, `recurrent_activation`, `recurrent_dropout`, `unroll`, `use_bias` y `reset_after`. Por eso he comentado `recurrent_dropout=0.2` (en comparación con el libro)."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Iee8VoknCVij","executionInfo":{"status":"ok","timestamp":1696448390034,"user_tz":180,"elapsed":752,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}}},"outputs":[],"source":["model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     #dropout=0.2, recurrent_dropout=0.2),\n","                     dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,\n","                     #dropout=0.2, recurrent_dropout=0.2),\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","])\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","# history = model.fit(dataset, epochs=10)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jmNzacFSCVij","executionInfo":{"status":"ok","timestamp":1696448392248,"user_tz":180,"elapsed":8,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"69967e74-08a6-4dc1-b1cd-fdad075c9f9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," gru (GRU)                   (None, None, 128)         74880     \n","                                                                 \n"," gru_1 (GRU)                 (None, None, 128)         99072     \n","                                                                 \n"," time_distributed (TimeDist  (None, None, 65)          8385      \n"," ributed)                                                        \n","                                                                 \n","=================================================================\n","Total params: 182337 (712.25 KB)\n","Trainable params: 182337 (712.25 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"YSiqszR9CVij","executionInfo":{"status":"error","timestamp":1696448397724,"user_tz":180,"elapsed":869,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"ef88b6c6-879d-4fb3-a585-881e0ee0e203"},"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-93f421229bda>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/NLP_char.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n","\u001b[0;31mOSError\u001b[0m: No file or directory found at models/NLP_char.h5"]}],"source":["model = keras.models.load_model('models/NLP_char.h5')"]},{"cell_type":"markdown","metadata":{"id":"oNHViCWpCVij"},"source":["## Usando el modelo para generar texto"]},{"cell_type":"markdown","metadata":{"id":"FUzaIbViCVij"},"source":["Construimos una pequeña función para preprocesar cualquier texto que pasemos al modelo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-SPH4lhCVij"},"outputs":[],"source":["def preprocess(texts):\n","    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n","    return tf.one_hot(X, max_id)"]},{"cell_type":"markdown","metadata":{"id":"kcgh2bfhCVij"},"source":["**Atención**: el método `predict_classes()` está obsoleto. En su lugar, debemos utilizar `np.argmax(model(X_new), axis=-1)`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-JZNV8uCVij"},"outputs":[],"source":["X_new = preprocess([\"The King is dea\"])\n","#Y_pred = model.predict_classes(X_new)\n","Y_pred = np.argmax(model(X_new), axis=-1)\n","\n","# Suma 1 porque los tokens van de 1 en adelante (el cero es para enmascarar)\n","tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpJSrCjNCVij"},"outputs":[],"source":["(np.argmax(model.predict(X_new), axis=-1) + 1)[0][-1]"]},{"cell_type":"markdown","metadata":{"id":"KnQuDDDRCVij"},"source":["¡Éxito! Juguemos con otros textos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnkaoxtwCVij"},"outputs":[],"source":["X_new = preprocess([\"It's only machine learning but I like i\"])\n","Y_pred = np.argmax(model(X_new), axis=-1)\n","tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"]},{"cell_type":"markdown","metadata":{"id":"T-d-sl5zCVij"},"source":["Produzcamos ahora una secuencia de caracteres, uno tras otro, para hacer nuevos textos completos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufkFIvFDCVij"},"outputs":[],"source":["tf.random.set_seed(42)\n","\n","tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkI2S-WuCVij"},"outputs":[],"source":["def next_char(text, temperature=1):\n","    X_new = preprocess([text])\n","    y_proba = model(X_new)[0, -1:, :]\n","    rescaled_logits = tf.math.log(y_proba) / temperature\n","    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n","    return tokenizer.sequences_to_texts(char_id.numpy())[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0PYfr99HCVij"},"outputs":[],"source":["tf.random.set_seed(42)\n","\n","next_char(\"How are yo\", temperature=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJnieOeBCVik"},"outputs":[],"source":["def complete_text(text, n_chars=50, temperature=1):\n","    for _ in range(n_chars):\n","        text += next_char(text, temperature)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JInUbAqCVik"},"outputs":[],"source":["tf.random.set_seed(42)\n","\n","print(complete_text(\"t\", n_chars=100, temperature=0.2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evrcJGyuCVik"},"outputs":[],"source":["print(complete_text(\"t\", temperature=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3icOjDabCVik"},"outputs":[],"source":["print(complete_text(\"t\", temperature=2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZGRWqm8uCVik"},"outputs":[],"source":["print(complete_text(shakespeare_text[:100], n_chars=100, temperature=0.2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVSMtwIKCVik"},"outputs":[],"source":["print(shakespeare_text[:100])"]},{"cell_type":"markdown","metadata":{"id":"n0VWk0_QCVik"},"source":["## * Stateful RNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzPLkYs6CVik"},"outputs":[],"source":["tf.random.set_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"ChQo2IroCVik"},"source":["Las RNN con estado (Stateful RNN) mantienen los valores de estado de una instancia a la siguiente, lo que permite aprender secuencias mucho más largas.\n","\n","El principal problema es la preparación del conjunto de datos, especialmente si queremos agrupar las instancias por lotes. Para evitar este problema, ahora utilizamos una única instancia por lote."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfFS7uN_CVik"},"outputs":[],"source":["dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n","dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","\n","# Cambiar esto requiere mucho trabajo!\n","dataset = dataset.batch(1)\n","dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","dataset = dataset.prefetch(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1SpFUeSCVik"},"outputs":[],"source":["batch_size = 32\n","encoded_parts = np.array_split(encoded[:train_size], batch_size)\n","datasets = []\n","for encoded_part in encoded_parts:\n","    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n","    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n","    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","    datasets.append(dataset)\n","dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n","dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","dataset = dataset.prefetch(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xw8RYHQ-CVik"},"outputs":[],"source":["model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, stateful=True,\n","                     #dropout=0.2, recurrent_dropout=0.2,\n","                     dropout=0.2,\n","                     batch_input_shape=[batch_size, None, max_id]),\n","    keras.layers.GRU(128, return_sequences=True, stateful=True,\n","                     #dropout=0.2, recurrent_dropout=0.2),\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","])"]},{"cell_type":"markdown","metadata":{"id":"LVX60r_tCVik"},"source":["Al final de cada época, queremos volver a los estados vacíos y empezar de nuevo. Esto se hace con un `Callback` casero."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uo6SaddWCVik"},"outputs":[],"source":["class ResetStatesCallback(keras.callbacks.Callback):\n","    def on_epoch_begin(self, epoch, logs):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V018uZOjCVik"},"outputs":[],"source":["model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","history = model.fit(dataset, epochs=50,\n","                    callbacks=[ResetStatesCallback()])"]},{"cell_type":"markdown","metadata":{"id":"7iAr4XUkCVik"},"source":["Para utilizar el modelo con diferentes tamaños de lote, necesitamos crear una copia sin estado. Podemos deshacernos del abandono, ya que sólo se utiliza durante el entrenamiento:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHIB3vUUCVik"},"outputs":[],"source":["stateless_model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n","    keras.layers.GRU(128, return_sequences=True),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","])"]},{"cell_type":"markdown","metadata":{"id":"Kq9Yh46KCVik"},"source":["Para fijar los pesos, primero tenemos que construir el modelo (para que se creen los pesos):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFMq-NlOCVik"},"outputs":[],"source":["stateless_model.build(tf.TensorShape([None, None, max_id]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pi7hAcINCVik"},"outputs":[],"source":["stateless_model.set_weights(model.get_weights())\n","model = stateless_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbwgPz24CVik"},"outputs":[],"source":["tf.random.set_seed(42)\n","\n","print(complete_text(\"t\"))"]}],"metadata":{"interpreter":{"hash":"2b7aa682480b82eb27ca7b5ecfebdb0027bb2a276e6bdff64c1ddeab03557e9e"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"nav_menu":{},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}